{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3767ea0f",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "In this laboratory, you will train an MLP model on a dataset of benign and DDoS network traffic using an automated mechanism to find the best hyper-parameters. This laboratory consists of a series of implementation steps:\n",
    "- add an early-stopping strategy to the grid-search mechanism\n",
    "- add more folds\n",
    "- implement a randomised search with sci-kit libraries using a continuous variable for the learning rate\n",
    "- compare Grid Search with Randomized Search in terms of execution time and accuracy of the best model on the validation set\n",
    "- replace the model architecture with a CNN and the relevant hyperparameters to tune (e.g., number of kernels, kernel height, max pooling, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aec2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Lecture on Intrusion Detection with Deep Learning\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import os\n",
    "import csv\n",
    "import h5py\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Seed Random Numbers\n",
    "SEED = 1\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "config = tf.compat.v1.ConfigProto(inter_op_parallelism_threads=1)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from itertools import cycle\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers import Dense, Activation,  Flatten, Conv2D\n",
    "from tensorflow.keras.layers import  GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model, Sequential, save_model, load_model, clone_model\n",
    "from sklearn.metrics import f1_score, precision_score, accuracy_score, log_loss, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "tf.random.set_seed(SEED)\n",
    "K.set_image_data_format('channels_last')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "#config.log_device_placement = True  # to log device placement (on which device the operation ran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37fcf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "MAX_EPOCHS=20\n",
    "hyperparamters = {\n",
    "    \"dense_layers\" : [1,2],\n",
    "    \"n_neurons\" : [16,32]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4f7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    filename = glob.glob(path)[0]\n",
    "    dataset = h5py.File(filename, \"r\")\n",
    "    set_x_orig = np.array(dataset[\"set_x\"][:])  # features\n",
    "    set_y_orig = np.array(dataset[\"set_y\"][:])  # labels\n",
    "\n",
    "    X = np.reshape(set_x_orig, (set_x_orig.shape[0], set_x_orig.shape[1], set_x_orig.shape[2], 1))\n",
    "    Y = set_y_orig\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27630a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "def MLPModel(dense_layers=2, n_neurons=32,learning_rate=0.01,input_shape=(10,11,1)):\n",
    "    K.clear_session()\n",
    "\n",
    "    model = Sequential(name  = \"mlp\")\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    for layer in range(dense_layers):\n",
    "        model.add(Dense(n_neurons, activation='relu', name='hidden-fc' + str(layer)))\n",
    "    model.add(Dense(1, activation='sigmoid', name='fc2'))\n",
    "    print(model.summary())\n",
    "    compileModel(model, learning_rate)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileModel(model,lr):\n",
    "    optimizer = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])  # here we specify the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c6f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = load_dataset(\"../Datasets/IDS2017/*\" + '-train.hdf5')\n",
    "X_val, Y_val = load_dataset(\"../Datasets/IDS2017/*\" + '-val.hdf5')\n",
    "\n",
    "X_train, Y_train = shuffle(X_train, Y_train, random_state=SEED)\n",
    "X_val, Y_val = shuffle(X_val, Y_val, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f3a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "keras_classifier = KerasClassifier(build_fn = MLPModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8349a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparameter search strategy\n",
    "rnd_search_cv = GridSearchCV(keras_classifier, hyperparamters, cv=2, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af450c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training process\n",
    "rnd_search_cv.fit(X_train, Y_train, epochs=MAX_EPOCHS, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e02c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results\n",
    "print(\"Train scores: \", rnd_search_cv.cv_results_)\n",
    "best_model = rnd_search_cv.best_estimator_.model\n",
    "Y_pred_val = (best_model.predict(X_val) > 0.5)\n",
    "Y_true_val = Y_val.reshape((Y_val.shape[0], 1))\n",
    "f1_score_val = f1_score(Y_true_val, Y_pred_val)\n",
    "\n",
    "print(\"\\nBest parameters: \", rnd_search_cv.best_params_)\n",
    "print(\"F1 Score of the best model on the validation set: \", f1_score_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f30fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
