{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e181315",
   "metadata": {},
   "source": [
    "# Anomaly detection with an Autoencoder\n",
    "In this laboratory, you will train an autoencoder with a dataset of benign traffic to make it learn the profile of the normal behaviour of the network. The current script will test the trained autoencoder on the test set of benign traffic, so that you can understand the maximum error produced by the trained autoencoder in reconstructing the normal traffic. \n",
    "\n",
    "This operation gives you the idea of what threshold can be set to detect anomalies (all the traffic flows whose reconstruction error is higher than the treshold are classified as anomalies). Copy the last cell of this notebook to test the autoencoder on the DOS2019 dataset of DDoS attacks. Set a threshold and compute the accuracy of the system using the F1 score metric.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b20e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Lecture on Intrusion Detection with Deep Learning\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import pprint\n",
    "import argparse\n",
    "\n",
    "import keras.callbacks\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import os\n",
    "import csv\n",
    "import h5py\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Seed Random Numbers\n",
    "SEED = 1\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "config = tf.compat.v1.ConfigProto(inter_op_parallelism_threads=1)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense,  Flatten, Conv2D, Reshape, Input, UpSampling2D\n",
    "from tensorflow.keras.layers import  MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model, Sequential, save_model, load_model, clone_model\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from scipy.stats import *\n",
    "from numpy.random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "tf.random.set_seed(SEED)\n",
    "K.set_image_data_format('channels_last')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea053bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(x_true,x_pred):\n",
    "    mse = ((x_true-x_pred)**2).mean(axis=(1,2))\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a20367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    filename = glob.glob(path)[0]\n",
    "    dataset = h5py.File(filename, \"r\")\n",
    "    set_x_orig = np.array(dataset[\"set_x\"][:])  # features\n",
    "    set_y_orig = np.array(dataset[\"set_y\"][:])  # labels\n",
    "\n",
    "    X = np.reshape(set_x_orig, (set_x_orig.shape[0], set_x_orig.shape[1], set_x_orig.shape[2], 1))\n",
    "    Y = set_y_orig\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97d1dc",
   "metadata": {},
   "source": [
    "## Performance metrics\n",
    "The following method computes the metrics to assess the performance of the models on the given datasets. Both accuracy and F1 Score are widely used metrics in many domains of the computer science. More information can be found in the ```sklearn``` documentation ([accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score), [F1 Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)) and in online documentation (e.g., [Metrics to Evaluate your Machine Learning Algorithm](https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88cb791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(Y_true, Y_pred):\n",
    "    Y_true = Y_true.reshape((Y_true.shape[0], 1))\n",
    "    accuracy = accuracy_score(Y_true, Y_pred)\n",
    "    f1 = f1_score(Y_true, Y_pred)\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407d7d9",
   "metadata": {},
   "source": [
    "## Autoencoder implementation\n",
    "It's worth noting that you should use the same input and output shape for your autoencoder's input and output layer, otherwise you will receive a shape error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f40c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AutoencoderMLP(input_shape=(10,11,1), learning_rate=0.01):\n",
    "    model = Sequential(name=\"autoencoder-mlp\")\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    model.add(Dense(256, activation='relu', name='enc0'))\n",
    "    model.add(Dense(32, activation='relu', name='enc1'))\n",
    "    model.add(Dense(8, activation='relu', name='enc2'))\n",
    "    model.add(Dense(8, activation='relu', name='dec0'))\n",
    "    model.add(Dense(32, activation='relu', name='dec1'))\n",
    "    model.add(Dense(256, activation='relu', name='dec2'))\n",
    "    model.add(Dense(input_shape[0] * input_shape[1], activation='sigmoid', name='dec3'))\n",
    "    model.add(Reshape(input_shape))\n",
    "\n",
    "    compileModel(model, learning_rate)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileModel(model,lr):\n",
    "    optimizer = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')  # here we specify the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7b7e1",
   "metadata": {},
   "source": [
    "## Training phase\n",
    "Here we train the autoencoder using a patience value of 10 epochs and a maximum numer of epochs set to 100. Note that, we do not need any labels in anomaly detection tasks, as the input and the expected output of an autoencoder are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, _ = load_dataset(\"../Datasets/HTTP-Benign/*benign*\" + '-train.hdf5')\n",
    "X_val, _ = load_dataset(\"../Datasets/HTTP-Benign/*benign*\" + '-val.hdf5')\n",
    "\n",
    "autoencoder = AutoencoderMLP()\n",
    "autoencoder.fit(X_train, X_train, batch_size=512, epochs=1000,validation_data=(X_val, X_val), callbacks=[EarlyStopping(monitor='val_loss',restore_best_weights=True, patience=25)])\n",
    "\n",
    "print(\"Saving best model's weights...\")\n",
    "autoencoder.save(\"./\" + autoencoder.name + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78d5c0",
   "metadata": {},
   "source": [
    "## Preliminary test phase\n",
    "The the trained autoencoder on the test set of benign traffic to verify the maximum error computed on the normal data. You can use this information to set the threshold for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0317084",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filelist = glob.glob(\"../Datasets/HTTP-Benign/*test.hdf5\")\n",
    "\n",
    "model_file = glob.glob(\".\" + \"/*.h5\")[0]\n",
    "print (\"Model: \", model_file)\n",
    "model = load_model(model_file)\n",
    "\n",
    "for dataset in dataset_filelist:\n",
    "    X_test, _ = load_dataset(dataset)\n",
    "    if \"benign\" in dataset:\n",
    "        # here we test the model on the test set of benign traffic\n",
    "        # Calculate the MSE loss between the original test data and the reconstructed test data\n",
    "        X_decoded = model.predict(X_test, batch_size=2048)\n",
    "        mse_loss = mean_squared_error(np.squeeze(X_test), np.squeeze(X_decoded))\n",
    "\n",
    "        print(\"MSE loss on test data:\", mse_loss)\n",
    "        n, bins, patches = plt.hist(mse_loss,bins=100)\n",
    "        plt.show()\n",
    "\n",
    "# set a threshold based on the mse distribution that you see in the plot\n",
    "THRESHOLD="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0302e0df",
   "metadata": {},
   "source": [
    "## Test with malicious traffic\n",
    "Here you extract the malicious traffic from a dataset and check the distribution of the mean squared error. This is possible only because we already have some malicious data. In real-world applications, realistic malicious data might not be available for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filelist = glob.glob(\"../Datasets/SYN2020/*test.hdf5\")\n",
    "\n",
    "model_file = glob.glob(\".\" + \"/*.h5\")[0]\n",
    "print (\"Model: \", model_file)\n",
    "model = load_model(model_file)\n",
    "\n",
    "for dataset in dataset_filelist:\n",
    "    X_test, Y_test = load_dataset(dataset)\n",
    "    X_test_malicious = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        if Y_test[i] == 1:\n",
    "            X_test_malicious.append(X_test[i])\n",
    "    X_test = np.array(X_test_malicious)\n",
    "    X_decoded = model.predict(X_test, batch_size=2048)\n",
    "    mse_loss = mean_squared_error(np.squeeze(X_test), np.squeeze(X_decoded))\n",
    "\n",
    "    n, bins, patches = plt.hist(mse_loss, bins=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c34fac2",
   "metadata": {},
   "source": [
    "## Test the threshold\n",
    "Now we evaluate the performance of our autoencoder by using the anomaly threshold that we set by looking at the mse distribution on the benign traffic. You can play with the threshold value and see how the accuracy scores are affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in dataset_filelist:\n",
    "    X_test, Y_test = load_dataset(dataset)\n",
    "    X_decoded = model.predict(X_test, batch_size=2048)\n",
    "    mse_loss = mean_squared_error(np.squeeze(X_test), np.squeeze(X_decoded))\n",
    "    Y_pred = np.squeeze(mse_loss > THRESHOLD)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred, labels=[0, 1]).ravel()\n",
    "    row = {'Samples': Y_test.shape[0], 'FPR': fp / (fp+tn), 'TNR': tn / (fp+tn), 'FNR': fn / (fn+tp), 'TPR': tp / (tp+fn), 'Model': model.name}\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
