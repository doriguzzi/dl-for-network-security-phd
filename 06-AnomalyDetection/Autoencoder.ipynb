{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe8d9cf3",
   "metadata": {},
   "source": [
    "# Anomaly detection with an Autoencoder\n",
    "In this laboratory, you will train an autoencoder with a dataset of benign traffic to make it learn the profile of the normal behaviour of the network. The current script will test the trained autoencoder on the test set of benign traffic, so that you can understand the maximum error produced by the trained autoencoder in reconstructing the normal traffic. \n",
    "\n",
    "This operation gives you the idea of what threshold can be set to detect anomalies (all the traffic flows whose reconstruction error is higher than the treshold are classified as anomalies). Copy the last cell of this notebook to test the autoencoder on the DOS2019 dataset of DDoS attacks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c7013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Lecture on Intrusion Detection with Deep Learning\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import pprint\n",
    "import argparse\n",
    "\n",
    "import keras.callbacks\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import os\n",
    "import csv\n",
    "import h5py\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Seed Random Numbers\n",
    "SEED = 1\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "config = tf.compat.v1.ConfigProto(inter_op_parallelism_threads=1)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense,  Flatten, Conv2D, Reshape, Input, UpSampling2D\n",
    "from tensorflow.keras.layers import  MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model, Sequential, save_model, load_model, clone_model\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from scipy.stats import *\n",
    "from numpy.random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "tf.random.set_seed(SEED)\n",
    "K.set_image_data_format('channels_last')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68150f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(x_true,x_pred):\n",
    "    mse = ((x_true-x_pred)**2).mean(axis=(1,2))\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69116e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    filename = glob.glob(path)[0]\n",
    "    dataset = h5py.File(filename, \"r\")\n",
    "    set_x_orig = np.array(dataset[\"set_x\"][:])  # features\n",
    "    set_y_orig = np.array(dataset[\"set_y\"][:])  # labels\n",
    "\n",
    "    X = np.reshape(set_x_orig, (set_x_orig.shape[0], set_x_orig.shape[1], set_x_orig.shape[2], 1))\n",
    "    Y = set_y_orig\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87012c",
   "metadata": {},
   "source": [
    "## Autoencoder implementation\n",
    "It's worth noting that you should use the same input and output shape for your autoencoder's input and output layer, otherwise you will receive a shape error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30048b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AutoencoderMLP(input_shape=(10,11,1), learning_rate=0.01):\n",
    "    model = Sequential(name=\"autoencoder-mlp\")\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    model.add(Dense(256, activation='relu', name='enc0'))\n",
    "    model.add(Dense(32, activation='relu', name='enc1'))\n",
    "    model.add(Dense(8, activation='relu', name='enc2'))\n",
    "    model.add(Dense(8, activation='relu', name='dec0'))\n",
    "    model.add(Dense(32, activation='relu', name='dec1'))\n",
    "    model.add(Dense(256, activation='relu', name='dec2'))\n",
    "    model.add(Dense(input_shape[0] * input_shape[1], activation='sigmoid', name='dec3'))\n",
    "    model.add(Reshape(input_shape))\n",
    "\n",
    "    compileModel(model, learning_rate)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3d1546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileModel(model,lr):\n",
    "    optimizer = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')  # here we specify the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d67e4",
   "metadata": {},
   "source": [
    "## Training phase\n",
    "Here we train the autoencoder using a patience value of 10 epochs and a maximum numer of epochs set to 100. Note that, we do not need any labels in anomaly detection tasks, as the input and the expected output of an autoencoder are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b512c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, _ = load_dataset(\"../Datasets/HTTP-Benign/*benign*\" + '-train.hdf5')\n",
    "X_val, _ = load_dataset(\"../Datasets/HTTP-Benign/*benign*\" + '-val.hdf5')\n",
    "\n",
    "autoencoder = AutoencoderMLP()\n",
    "autoencoder.fit(X_train, X_train, batch_size=512, epochs=1000,validation_data=(X_val, X_val), callbacks=[EarlyStopping(monitor='val_loss',restore_best_weights=True, patience=25)])\n",
    "\n",
    "print(\"Saving best model's weights...\")\n",
    "autoencoder.save(\"./\" + autoencoder.name + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdf94a9",
   "metadata": {},
   "source": [
    "## Preliminary test phase\n",
    "The the trained autoencoder on the test set of benign traffic to verify the maximum error computed on the normal data. This information can be used to set the threshold for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d29542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_filelist = glob.glob(\"../Datasets/HTTP-Benign/*test.hdf5\")\n",
    "\n",
    "model_file = glob.glob(\".\" + \"/*.h5\")[0]\n",
    "print (\"Model: \", model_file)\n",
    "model = load_model(model_file)\n",
    "\n",
    "for dataset in dataset_filelist:\n",
    "    X_test, _ = load_dataset(dataset)\n",
    "    if \"benign\" in dataset:\n",
    "        # here we test the model on the test set of benign traffic\n",
    "        # Calculate the MSE loss between the original test data and the reconstructed test data\n",
    "        X_decoded = model.predict(X_test, batch_size=2048)\n",
    "        mse_loss = mean_squared_error(np.squeeze(X_test), np.squeeze(X_decoded))\n",
    "\n",
    "        print(\"MSE loss on test data:\", mse_loss)\n",
    "        n, bins, patches = plt.hist(mse_loss,bins=100)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d69b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
